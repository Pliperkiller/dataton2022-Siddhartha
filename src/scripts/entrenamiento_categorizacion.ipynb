{"cells":[{"cell_type":"markdown","metadata":{"id":"TLwD7yExtuYx"},"source":["# Documentación"]},{"cell_type":"markdown","metadata":{"id":"WHQyEI3ttuY2"},"source":["Se llaman las librerías que serán utilizadas. Entre ellas dos librerías propias las cuales son:\n","\n","- url_config: Esta libreria contiene una función a través de la cual se realiza el webscrapping de varios periodicos en línea.\n","- utilitools: Libreria de varios usos, se usa para configurar el arbol de rutas particularmente. "]},{"cell_type":"code","execution_count":1,"metadata":{"id":"joXEJQSGtuY3"},"outputs":[],"source":["import requests\n","import re\n","import nltk\n","import random\n","import pandas as pd\n","from nltk import FreqDist\n","from nltk import word_tokenize\n","from nltk.tokenize import RegexpTokenizer\n","from nltk.corpus import stopwords\n","from nltk.util import ngrams\n","from bs4 import BeautifulSoup\n","from urllib import request\n","from url_config import *\n","from utilitools import *"]},{"cell_type":"markdown","metadata":{"id":"kbLGGzYbuWzl"},"source":["Creamos las siguientes funciones para el proceso de entrenamiento y el proceso de tokenización de las noticias."]},{"cell_type":"code","execution_count":61,"metadata":{"id":"0SSHqJoXtuY7"},"outputs":[],"source":["# Esta función regresa los tokens del cuerpo de la noticia\n","def tokenizar_url(url):\n","    #Se envia solicitud a la página\n","    resultado=requests.get(url)\n","\n","    #Se solicita el texo\n","    content=resultado.text\n","    soup=BeautifulSoup(content, 'lxml')\n","\n","    if 'espectador' in url.lower(): box = bs_espectador(soup)\n","    elif 'semana' in url.lower(): box = bs_semana(soup)\n","    elif 'larepublica' in url.lower(): box = bs_larepublica(soup)\n","    elif 'portafolio' in url.lower(): box = bs_portafolio(soup)\n","\n","    else: return 'no config'\n","\n","    tokenizer=RegexpTokenizer('\\w+')\n","    tokens=tokenizer.tokenize(box)\n","    tokens=[token.lower() for token in tokens]\n","\n","    return tokens\n","\n","# Esta función regresa un top de las palabras mas utilizadas dentro de las noticias, por defecto defecto devuelve el top 100\n","def top_freqdst(tokens,n=100):\n","    stops = stopwords.words('spanish')\n","\n","    filtered_tokens = [token for token in tokens if token not in stops]\n","\n","    f_dst = FreqDist(filtered_tokens)\n","    top_n = f_dst.most_common(n)\n","\n","    return (top_n)\n","\n","# Calcula la riqueza léxica del cuerpo de la noticia. Este calculo se realiza con el total de palabras diferentes dividido el total de palabras\n","def riq_lex(tokens):\n","    vocabulario = sorted(set(tokens))\n","    return len(vocabulario)/len(tokens)\n","\n","# Retorna el top de n-gramas, es decir, las n agrupaciones de palabras que componen el contenido del texto. Por defecto devuelve el top 100 de bi-gramas\n","def top_ngrams_freqdist(tokens,n=2,m=100):\n","    n_grams = ngrams(tokens, n)\n","    \n","    token_grams = [ ' '.join(elms) for elms in n_grams]\n","    \n","    f_dst = FreqDist(token_grams)\n","    top_m = f_dst.most_common(m) \n","    return (top_m)\n","\n","# Esta función se utiliza para agrupar los parametros de entrenamiento del modelo de clasificación Bayesiano\n","def atributos(tokens):\n","    atrib = {}\n","    \n","    top_freqdist = top_freqdst(tokens,n=100)\n","    for x in top_freqdist:\n","        atrib['top_freqdist({})'.format(x[0])] = x[1]\n","    \n","    n_gram_freqdist = top_ngrams_freqdist(tokens,n=2,m=100)\n","    for x in n_gram_freqdist:\n","        atrib['top_ngram({})'.format(x[0])] = x[1]\n","\n","    #atrib['riq_lex'] = riq_lex(tokens)\n","\n","    return atrib\n"]},{"cell_type":"markdown","metadata":{"id":"TA6EFkfqwLyM"},"source":["Se realiza la lectura del arbol de archivos del proyecto. "]},{"cell_type":"code","execution_count":3,"metadata":{"id":"5JXZ2_j8tuY9"},"outputs":[],"source":["data = leer_paths('data')\n","recomendador = leer_paths('recomendador')\n","scripts = leer_paths('scripts')"]},{"cell_type":"markdown","metadata":{"id":"nr6j6EJhwV5z"},"source":["Se procede a cargar como dataframe un excel que contiene las URL clasificadas previamente, las cuales serán utilizadas como entrenamiento. Adicionalmente, se realiza el proceso de tokenización del cuerpo de las noticias"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"3ei28zPNwUIj"},"outputs":[],"source":["df = pd.read_excel(os.path.join(data,'archivos_auxiliares\\\\url_noticias_clasificadas.xlsx'))\n","df = df[['url','categoria']]\n","df['tokens'] = df['url'].apply(tokenizar_url)"]},{"cell_type":"markdown","metadata":{"id":"HYxBb0howkhd"},"source":["Se realiza el entrenamiento del modelo de clasificación Bayesiana. Inicialmente se cuenta con un total de 18 noticias, las cuales corresponden a 3 noticias por cada categoría. Para el proceso de entrenamiento, se tendrán en cuenta 10 de ellas y las 8 restantes se usan el testeo. Por otra parte, se realiza la medida del accuracy"]},{"cell_type":"code","execution_count":57,"metadata":{"id":"6vqDEnrTtuY-","outputId":"52b57dbe-34b2-4285-c878-6d9ffb13257c"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.625\n"]}],"source":["fset = [(atributos(texto), clase) for texto, clase in zip(df['tokens'].values, df['categoria'].values)]\n","random.shuffle(fset)\n","train, test = fset[:10], fset[10:]\n","classifier = nltk.NaiveBayesClassifier.train(train)\n","print(nltk.classify.accuracy(classifier, test)) #se mide el accuracy"]},{"cell_type":"markdown","metadata":{"id":"L9D6S_bZxwjW"},"source":["El accuracy mas alto obtenido con las condiciones dadas, es de 0.625, sin embargo, debemos tener en cuenta que actualmente el valor fluctua debido a que la muestra de entrenamiento es muy pequeña y esto genera discrepancias en cada re entrenamiento. Esto se puede solucionar, teniendo una base de noticias clasificadas mas grande"]},{"cell_type":"markdown","metadata":{"id":"BbBxGqJkyVNz"},"source":["Una vez entrenado el modelo, procedemos a cargar el archivo que contiene las noticias"]},{"cell_type":"code","execution_count":58,"metadata":{"id":"1ATf9y8ptuZA"},"outputs":[],"source":["# Esta función carga el archivo .csv que se encuentra en la ruta path con el nombre csv_file como un dataframe, \n","def leer_csv(path,csv_file):\n","    '''\n","    Inserta ruta de carpeta donde se encuentra el archivo y nombre de archivo para hacer lectura de archivo como DataFrame\n","    '''\n","    for file in os.listdir(path):\n","        file_path = os.path.join(path,file)\n","\n","        if csv_file in file_path:\n","            df = pd.read_csv(file_path)\n","    return df\n","\n","df_noticias = leer_csv(data,'noticias.csv')\n"]},{"cell_type":"markdown","metadata":{"id":"DGoLqj06zB-a"},"source":["Se agrega la columna que contiene los tokens del cuerpo de la noticia"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"6M6jdh2RtuZB","outputId":"ca34366f-ae72-45d7-ca76-19b640cadb53"},"outputs":[],"source":["df_noticias['tokens'] = df_noticias['news_text_content'].apply(word_tokenize)"]},{"cell_type":"markdown","metadata":{"id":"Y7G_XL0Y3POy"},"source":["Se agrega una columna donde se clasifica la noticia, utilizando el clasificador entrenado"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"iV_7du95tuZB","outputId":"43e6e71a-0cf5-4b5a-8a5b-2aaf28c7b569"},"outputs":[],"source":["df_noticias['clase'] = df_noticias['tokens'].apply(lambda x: classifier.classify(atributos(x)))"]},{"cell_type":"markdown","metadata":{"id":"_jbgQoQe3hwJ"},"source":["Se exportan los datos"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"JLBUlnV7tuZC"},"outputs":[],"source":["df_noticias.drop(['tokens','news_title','news_text_content'], axis=1,inplace=True)\n","df_noticias.to_excel(os.path.join(data,'output\\\\categorizacion.xlsx'),index=False)"]}],"metadata":{"colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.10.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"}}},"nbformat":4,"nbformat_minor":0}
